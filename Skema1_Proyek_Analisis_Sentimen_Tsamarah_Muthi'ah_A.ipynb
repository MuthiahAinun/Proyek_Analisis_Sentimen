{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq6Ra9MTPPFA2Hh8G9RdjA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuthiahAinun/Proyek_Analisis_Sentimen/blob/main/Skema1_Proyek_Analisis_Sentimen_Tsamarah_Muthi'ah_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyek Analisis Sentimen: [Youtube-Comments-dataset]\n",
        "- **Nama:** [Tsamarah Muthi'ah Abdullah]\n",
        "- **Email:** [a135xaf486@devacademy.id]\n",
        "- **ID Dicoding:** [a135xaf48]"
      ],
      "metadata": {
        "id": "TGyNMBnyGxsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "PkjP4X-lHP1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menginstall google API\n",
        "!pip install --upgrade google-api-python-client pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "fUh0dCwlHnD7",
        "outputId": "0972e66b-bf1a-4287-f870-93a9f97bd38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.160.0)\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-2.165.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.69.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
            "Downloading google_api_python_client-2.165.0-py2.py3-none-any.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, google-api-python-client\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.160.0\n",
            "    Uninstalling google-api-python-client-2.160.0:\n",
            "      Successfully uninstalled google-api-python-client-2.160.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-api-python-client-2.165.0 pandas-2.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "googleapiclient"
                ]
              },
              "id": "89e98e9bd9e6453fbc24c427798b843d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwkllzT7zE1i",
        "outputId": "9cfc7d71-7a71-4aa4-f274-6dc6626dbce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import googleapiclient.discovery\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Conv1D, GlobalMaxPooling1D, Bidirectional, BatchNormalization\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "I6Pw9OLlHXG7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing dan Pembersihan"
      ],
      "metadata": {
        "id": "ENIdk__BiOJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membaca dan Menampilkan informasi dataset\n",
        "dataset = pd.read_csv(\"NDsO1LT_0lw_youtube_comments.csv\")\n",
        "print(\"Jumlah data:\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtg0gpXcjEH8",
        "outputId": "989917ae-6335-45eb-cf86-b19a94806ef5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data: 85028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ADwNxWVjWUx",
        "outputId": "43ecaad9-c519-4c54-ac8f-2d10b5d6773c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             username                                               text  \\\n",
            "0            @MrBeast  BEAST GAMES FINALE DROPS FEBRUARY 13TH! GO WAT...   \n",
            "1     @SultanRayyan-7                          Saya orang Indonesia ğŸ‡®ğŸ‡©ğŸ‡®ğŸ‡©   \n",
            "2  @MohamadrezaRezayy  Ù…Ø³ØªØ± Ø¨ÛŒØ³Øª Ø¹Ø²ÛŒØ² â¤ Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§ÛŒÙ† ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§...   \n",
            "3       @ILoveyou-954  à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦†à¦¸à¦¬à§‡à¦¨ à¦•à¦¬à§‡ à¦ªà¦¾à¦¬à¦¨à¦¾ à¦œà§‡à¦²à¦¾à¦° à¦•à¦¿à¦›à§ à¦¦à§‡à¦–à¦¤à§‡ à¦šà¦¾à¦‡ ...   \n",
            "4     @Moneymakerarab  This is for girls listen to Dalida - Helwa ya ...   \n",
            "\n",
            "   likes          published_at  \n",
            "0  54600  2025-02-08T16:59:31Z  \n",
            "1      0  2025-03-19T21:51:42Z  \n",
            "2      0  2025-03-19T21:43:03Z  \n",
            "3      0  2025-03-19T21:34:32Z  \n",
            "4      0  2025-03-19T21:20:21Z  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Informasi Dataset:\")\n",
        "print(dataset.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD1WeBk3jj4g",
        "outputId": "16f613eb-1838-4ade-e902-10fa235e1b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informasi Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 85028 entries, 0 to 85027\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   username      84982 non-null  object\n",
            " 1   text          85028 non-null  object\n",
            " 2   likes         85028 non-null  int64 \n",
            " 3   published_at  85028 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 2.6+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nStatistik Deskriptif:')\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilYCEbrKjv5n",
        "outputId": "0a992fb2-a75d-4d7d-b337-9c4e3e97cb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistik Deskriptif:\n",
            "               likes\n",
            "count   85028.000000\n",
            "mean        9.727654\n",
            "std       744.702565\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         1.000000\n",
            "max    167348.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek jumlah missing values di setiap kolom\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEaqaxe4j6Li",
        "outputId": "8b2da518-70f6-484b-feb4-6c0df3f6c85c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "username        46\n",
            "text             0\n",
            "likes            0\n",
            "published_at     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus nilai kosong\n",
        "dataset.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "ZTNC1ghpkkpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus duplikasi\n",
        "dataset.drop_duplicates(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "b5HYvwY0kn00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pembersihan teks\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join(c for c in text if c.isalnum() or c.isspace())\n",
        "    return text\n",
        "\n",
        "dataset['cleaned_text'] = dataset['text'].apply(clean_text)\n",
        "print(\"Data setelah pembersihan:\")\n",
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mi_m65UkrwQ",
        "outputId": "26fa3624-060f-4202-db4e-a25025966e5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data setelah pembersihan:\n",
            "             username                                               text  \\\n",
            "0            @MrBeast  BEAST GAMES FINALE DROPS FEBRUARY 13TH! GO WAT...   \n",
            "1     @SultanRayyan-7                          Saya orang Indonesia ğŸ‡®ğŸ‡©ğŸ‡®ğŸ‡©   \n",
            "2  @MohamadrezaRezayy  Ù…Ø³ØªØ± Ø¨ÛŒØ³Øª Ø¹Ø²ÛŒØ² â¤ Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§ÛŒÙ† ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§...   \n",
            "3       @ILoveyou-954  à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦†à¦¸à¦¬à§‡à¦¨ à¦•à¦¬à§‡ à¦ªà¦¾à¦¬à¦¨à¦¾ à¦œà§‡à¦²à¦¾à¦° à¦•à¦¿à¦›à§ à¦¦à§‡à¦–à¦¤à§‡ à¦šà¦¾à¦‡ ...   \n",
            "4     @Moneymakerarab  This is for girls listen to Dalida - Helwa ya ...   \n",
            "\n",
            "   likes          published_at  \\\n",
            "0  54600  2025-02-08T16:59:31Z   \n",
            "1      0  2025-03-19T21:51:42Z   \n",
            "2      0  2025-03-19T21:43:03Z   \n",
            "3      0  2025-03-19T21:34:32Z   \n",
            "4      0  2025-03-19T21:20:21Z   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  beast games finale drops february 13th go watc...  \n",
            "1                              saya orang indonesia   \n",
            "2  Ù…Ø³ØªØ± Ø¨ÛŒØ³Øª Ø¹Ø²ÛŒØ²  Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§ÛŒÙ† ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§ ...  \n",
            "3                    à¦¬à¦²à¦¦à¦¶ à¦†à¦¸à¦¬à¦¨ à¦•à¦¬ à¦ªà¦¬à¦¨ à¦œà¦²à¦° à¦•à¦› à¦¦à¦–à¦¤ à¦šà¦‡   \n",
            "4  this is for girls listen to dalida  helwa ya b...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek kembali jumlah missing values di setiap kolom\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocQOJSqMk5Nv",
        "outputId": "8de896c9-aebe-45d7-a35c-9bdd3b8b9bac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "username        0\n",
            "text            0\n",
            "likes           0\n",
            "published_at    0\n",
            "cleaned_text    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Ekstraksi Fitur dan Pelabelan Data**"
      ],
      "metadata": {
        "id": "nBwOOPI5lASL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metode yang digunakan bebas sesuai dengan preferensi masing-masing peserta. Tahapan ini penting untuk mempersiapkan data sehingga dapat diolah lebih lanjut dalam proses pelatihan model."
      ],
      "metadata": {
        "id": "eJv-MtsUlHzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melabeli komentar secara manual (positif, netral, negatif)\n",
        "def label_comment(text):\n",
        "    if any(word in text for word in [\"love\", \"great\", \"awesome\", \"good\", \"nice\", \"amazing\", \"fantastic\"]):\n",
        "        return \"positif\"\n",
        "    elif any(word in text for word in [\"hate\", \"bad\", \"awful\", \"worst\", \"terrible\", \"disgusting\", \"boring\"]):\n",
        "        return \"negatif\"\n",
        "    else:\n",
        "        return \"netral\"\n",
        "\n",
        "dataset['label'] = dataset['cleaned_text'].apply(label_comment)\n",
        "print(\"Distribusi label:\")\n",
        "print(dataset['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Uvoet4lI9T",
        "outputId": "3bce9178-ec0a-4862-902d-631ef0faca56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi label:\n",
            "label\n",
            "netral     63766\n",
            "positif     4728\n",
            "negatif       69\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding label\n",
        "label_encoder = LabelEncoder()\n",
        "dataset['label_encoded'] = label_encoder.fit_transform(dataset['label'])"
      ],
      "metadata": {
        "id": "IqZS3KMplvFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Pembangunan Model Deep Learning**"
      ],
      "metadata": {
        "id": "97mjHYpAmZKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pilihan algoritma pelatihan ini haruslah sesuai dengan tujuan analisis sentimen yang ingin dicapai."
      ],
      "metadata": {
        "id": "lXa9RkwLmd-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skema 1: LSTM dengan Tokenizer + Padding (80/20)"
      ],
      "metadata": {
        "id": "5-Ui-xoToXXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algoritma:** Long Short-Term Memory (LSTM)\n",
        "\n",
        "**Ekstraksi Fitur:** Tokenizer + Padding\n",
        "\n",
        "**Alasan Pemilihan:**\n",
        "- LSTM sangat efektif dalam memproses data berbasis urutan, seperti teks atau kalimat.\n",
        "- Algoritma ini cocok untuk menangani long-term dependencies dalam urutan kata.\n",
        "- Tokenizer digunakan untuk mengonversi kata menjadi indeks numerik.\n",
        "- Padding digunakan untuk memastikan panjang urutan tetap sama agar dapat diproses dalam batch."
      ],
      "metadata": {
        "id": "1jBs_D849Vgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenisasi teks dengan maksimal 10.000 kata\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(dataset['cleaned_text'])"
      ],
      "metadata": {
        "id": "qyD12-NToddT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengubah teks menjadi urutan token\n",
        "sequences = tokenizer.texts_to_sequences(dataset['cleaned_text'])"
      ],
      "metadata": {
        "id": "OLS-lHtb_zeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Padding urutan agar memiliki panjang yang sama\n",
        "padded = pad_sequences(sequences, maxlen=100, padding='post')"
      ],
      "metadata": {
        "id": "TRM7lAwJ_220"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatasi Ketidakseimbangan Data dengan SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(padded, dataset['label_encoded'])"
      ],
      "metadata": {
        "id": "hHnttOdfRyE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membagi data menjadi data latih dan data uji (80/20)\n",
        "X_train, X_test, y_train, y_test = train_test_split(padded, dataset['label_encoded'], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "sKqC4BEiojg9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Membangun Model\n",
        "print(\"\\nSkema 1: LSTM dengan Tokenizer + Padding (80/20)\")\n",
        "\n",
        "class_weights = {0: 1000, 1: 1, 2: 10}\n",
        "\n",
        "model1 = Sequential([\n",
        "    Embedding(10000, 128, input_length=100),\n",
        "    Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09cS97VoonCA",
        "outputId": "94037db0-4904-4aa1-c1bb-1fab274c4a31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skema 1: LSTM dengan Tokenizer + Padding (80/20)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengompilasi model dengan optimizer Adam\n",
        "model1.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "CR-EvRDvAE0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih model dengan data latih\n",
        "model1.fit(X_train, y_train, epochs=30, validation_data=(X_test, y_test), batch_size=64, class_weight=class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4gCDo28AFZb",
        "outputId": "b59fd61b-8ee6-445e-80fe-45672bf7cebe"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m621s\u001b[0m 711ms/step - accuracy: 0.7042 - loss: 3.0077 - val_accuracy: 0.6939 - val_loss: 0.9136\n",
            "Epoch 2/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 727ms/step - accuracy: 0.7162 - loss: 2.0105 - val_accuracy: 0.6921 - val_loss: 0.6790\n",
            "Epoch 3/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m618s\u001b[0m 720ms/step - accuracy: 0.6885 - loss: 1.5884 - val_accuracy: 0.6890 - val_loss: 0.5883\n",
            "Epoch 4/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 725ms/step - accuracy: 0.7220 - loss: 1.2107 - val_accuracy: 0.7147 - val_loss: 0.5296\n",
            "Epoch 5/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m624s\u001b[0m 727ms/step - accuracy: 0.7228 - loss: 1.1010 - val_accuracy: 0.7704 - val_loss: 0.4286\n",
            "Epoch 6/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 722ms/step - accuracy: 0.7777 - loss: 0.8334 - val_accuracy: 0.8156 - val_loss: 0.3570\n",
            "Epoch 7/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m622s\u001b[0m 721ms/step - accuracy: 0.8497 - loss: 0.6095 - val_accuracy: 0.9255 - val_loss: 0.1743\n",
            "Epoch 8/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m619s\u001b[0m 722ms/step - accuracy: 0.9092 - loss: 0.5207 - val_accuracy: 0.9214 - val_loss: 0.1988\n",
            "Epoch 9/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m606s\u001b[0m 707ms/step - accuracy: 0.9185 - loss: 0.3999 - val_accuracy: 0.9365 - val_loss: 0.1589\n",
            "Epoch 10/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m609s\u001b[0m 710ms/step - accuracy: 0.9455 - loss: 0.3145 - val_accuracy: 0.9507 - val_loss: 0.1437\n",
            "Epoch 11/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m637s\u001b[0m 727ms/step - accuracy: 0.9471 - loss: 0.3682 - val_accuracy: 0.9543 - val_loss: 0.1436\n",
            "Epoch 12/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 728ms/step - accuracy: 0.9574 - loss: 0.2428 - val_accuracy: 0.9505 - val_loss: 0.1453\n",
            "Epoch 13/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 726ms/step - accuracy: 0.9483 - loss: 0.3790 - val_accuracy: 0.9643 - val_loss: 0.1181\n",
            "Epoch 14/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 709ms/step - accuracy: 0.9564 - loss: 0.3725 - val_accuracy: 0.9687 - val_loss: 0.1047\n",
            "Epoch 15/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m636s\u001b[0m 727ms/step - accuracy: 0.9719 - loss: 0.2641 - val_accuracy: 0.9392 - val_loss: 0.1833\n",
            "Epoch 16/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 731ms/step - accuracy: 0.9577 - loss: 0.3544 - val_accuracy: 0.9509 - val_loss: 0.1561\n",
            "Epoch 17/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 727ms/step - accuracy: 0.9636 - loss: 0.2407 - val_accuracy: 0.9015 - val_loss: 0.1884\n",
            "Epoch 18/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 728ms/step - accuracy: 0.9512 - loss: 0.2958 - val_accuracy: 0.8943 - val_loss: 0.2089\n",
            "Epoch 19/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 726ms/step - accuracy: 0.9416 - loss: 0.3132 - val_accuracy: 0.8894 - val_loss: 0.2275\n",
            "Epoch 20/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m625s\u001b[0m 728ms/step - accuracy: 0.9499 - loss: 0.3596 - val_accuracy: 0.9450 - val_loss: 0.1809\n",
            "Epoch 21/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m626s\u001b[0m 729ms/step - accuracy: 0.9764 - loss: 0.2026 - val_accuracy: 0.8862 - val_loss: 0.2491\n",
            "Epoch 22/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 734ms/step - accuracy: 0.9490 - loss: 0.2488 - val_accuracy: 0.8663 - val_loss: 0.3157\n",
            "Epoch 23/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 729ms/step - accuracy: 0.9681 - loss: 0.1524 - val_accuracy: 0.8935 - val_loss: 0.2383\n",
            "Epoch 24/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m679s\u001b[0m 725ms/step - accuracy: 0.9667 - loss: 0.2711 - val_accuracy: 0.9212 - val_loss: 0.2612\n",
            "Epoch 25/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m627s\u001b[0m 731ms/step - accuracy: 0.9595 - loss: 0.2892 - val_accuracy: 0.8743 - val_loss: 0.3003\n",
            "Epoch 26/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 732ms/step - accuracy: 0.9659 - loss: 0.3035 - val_accuracy: 0.8569 - val_loss: 0.3960\n",
            "Epoch 27/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m683s\u001b[0m 734ms/step - accuracy: 0.9576 - loss: 0.2197 - val_accuracy: 0.9214 - val_loss: 0.3137\n",
            "Epoch 28/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 732ms/step - accuracy: 0.9640 - loss: 0.1890 - val_accuracy: 0.9383 - val_loss: 0.2425\n",
            "Epoch 29/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m629s\u001b[0m 733ms/step - accuracy: 0.9693 - loss: 0.2479 - val_accuracy: 0.9176 - val_loss: 0.3260\n",
            "Epoch 30/30\n",
            "\u001b[1m858/858\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m680s\u001b[0m 730ms/step - accuracy: 0.9573 - loss: 0.2323 - val_accuracy: 0.9372 - val_loss: 0.2435\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7aa6723da910>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluasi Model Skema 1\n",
        "print(\"\\nEvaluasi Model Skema 1: LSTM dengan Tokenizer + Padding (80/20)\")\n",
        "\n",
        "# Prediksi pada data uji\n",
        "y_pred1 = np.argmax(model1.predict(X_test), axis=1)\n",
        "\n",
        "# Laporan Klasifikasi\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred1, target_names=label_encoder.classes_))\n",
        "\n",
        "# Akurasi\n",
        "accuracy = accuracy_score(y_test, y_pred1)\n",
        "print(f\"Akurasi Model: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebSLvWM6OD9J",
        "outputId": "0953bd95-b5a1-4b0a-ecf6-7918d69df640"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Evaluasi Model Skema 1: LSTM dengan Tokenizer + Padding (80/20)\n",
            "\u001b[1m429/429\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 92ms/step\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     negatif       0.01      0.60      0.01        10\n",
            "      netral       1.00      0.93      0.97     12792\n",
            "     positif       0.97      0.97      0.97       911\n",
            "\n",
            "    accuracy                           0.94     13713\n",
            "   macro avg       0.66      0.84      0.65     13713\n",
            "weighted avg       1.00      0.94      0.97     13713\n",
            "\n",
            "Akurasi Model: 0.9372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference dan Testing Skema 1\n",
        "print(\"\\nInference Skema 1: LSTM dengan Tokenizer + Padding (80/20)\")\n",
        "# Fungsi Prediksi Teks\n",
        "def predict_text_lstm(text, tokenizer, model1, label_encoder):\n",
        "    # Tokenisasi dan padding teks\n",
        "    sequence = tokenizer.texts_to_sequences([text])\n",
        "    padded_sequence = pad_sequences(sequence, maxlen=100, padding='post')\n",
        "\n",
        "    # Prediksi sentimen\n",
        "    prediction = model1.predict(padded_sequence)\n",
        "    label_index = np.argmax(prediction, axis=1)[0]\n",
        "    label = label_encoder.inverse_transform([label_index])[0]\n",
        "\n",
        "    print(f\"Teks: {text}\")\n",
        "    print(f\"Prediksi Sentimen: {label}\")\n",
        "\n",
        "# Prediksi Teks Baru\n",
        "print(\"\\nPrediksi Teks Baru:\")\n",
        "predict_text_lstm(\"I love your video!\", tokenizer, model1, label_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6h-PWXlhM2e",
        "outputId": "171758fd-f767-4758-948b-b51cc496ed34"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference Skema 1: LSTM dengan Tokenizer + Padding (80/20)\n",
            "\n",
            "Prediksi Teks Baru:\n",
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "Teks: I love your video!\n",
            "Prediksi Sentimen: positif\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight Skema 1:**\n",
        "1. Model ini memiliki akurasi yang tinggi, mencapai 93.72%. Hal ini menunjukkan bahwa model dapat mengklasifikasikan sebagian besar teks dengan benar.\n",
        "2. Precision untuk kelas \"negatif\" sangat rendah (0.01) meskipun recall cukup tinggi (0.60). Ini menunjukkan bahwa model sering salah mengklasifikasikan teks negatif, mungkin karena data negatif sangat sedikit atau model terlalu fokus pada kelas \"netral\" dan \"positif\".\n",
        "3. Precision, recall, dan f1-score pada kelas \"netral\" dan \"positif\" sangat tinggi (mendekati 1.00). Hal ini menunjukkan bahwa model mampu mengenali kedua sentimen tersebut dengan sangat baik.\n",
        "4. Model masih memiliki kelemahan pada kelas negatif, sehingga peningkatan dapat dilakukan dengan teknik augmentasi atau modifikasi arsitektur.\n",
        "5. Pada teks uji \"I love your video!\", model berhasil mengklasifikasikannya sebagai \"positif\", yang merupakan prediksi yang tepat.\n"
      ],
      "metadata": {
        "id": "PrDGNVEixe5m"
      }
    }
  ]
}