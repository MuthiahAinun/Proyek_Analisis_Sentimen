{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuthiahAinun/Proyek_Analisis_Sentimen/blob/main/Skema3_Proyek_Analisis_Sentimen_Tsamarah_Muthi'ah_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Proyek Analisis Sentimen: [Youtube-Comments-dataset]\n",
        "- **Nama:** [Tsamarah Muthi'ah Abdullah]\n",
        "- **Email:** [a135xaf486@devacademy.id]\n",
        "- **ID Dicoding:** [a135xaf48]"
      ],
      "metadata": {
        "id": "TGyNMBnyGxsv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Library"
      ],
      "metadata": {
        "id": "PkjP4X-lHP1v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Menginstall google API\n",
        "!pip install --upgrade google-api-python-client pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 931
        },
        "id": "fUh0dCwlHnD7",
        "outputId": "0972e66b-bf1a-4287-f870-93a9f97bd38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-api-python-client in /usr/local/lib/python3.11/dist-packages (2.160.0)\n",
            "Collecting google-api-python-client\n",
            "  Downloading google_api_python_client-2.165.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: httplib2<1.0.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.22.0)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (0.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (2.24.2)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.69.1)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (4.25.6)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (1.26.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.32.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client) (3.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=1.32.0->google-api-python-client) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0,>=1.31.5->google-api-python-client) (2025.1.31)\n",
            "Downloading google_api_python_client-2.165.0-py2.py3-none-any.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pandas, google-api-python-client\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "  Attempting uninstall: google-api-python-client\n",
            "    Found existing installation: google-api-python-client 2.160.0\n",
            "    Uninstalling google-api-python-client-2.160.0:\n",
            "      Successfully uninstalled google-api-python-client-2.160.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed google-api-python-client-2.165.0 pandas-2.2.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "googleapiclient"
                ]
              },
              "id": "89e98e9bd9e6453fbc24c427798b843d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FwkllzT7zE1i",
        "outputId": "9cfc7d71-7a71-4aa4-f274-6dc6626dbce9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.14.1\n",
            "    Uninstalling scipy-1.14.1:\n",
            "      Successfully uninstalled scipy-1.14.1\n",
            "Successfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "MH9fSYs2KsWi",
        "outputId": "d7a4c1a1-bbf6-42af-9b0a-66652f337d32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.5.1)\n",
            "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
            "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading tensorflow-2.19.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m980.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_io_gcs_filesystem-0.37.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m108.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: libclang, flatbuffers, wheel, werkzeug, tensorflow-io-gcs-filesystem, tensorboard-data-server, google-pasta, tensorboard, astunparse, tensorflow\n",
            "Successfully installed astunparse-1.6.3 flatbuffers-25.2.10 google-pasta-0.2.0 libclang-18.1.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 werkzeug-3.1.3 wheel-0.45.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn"
      ],
      "metadata": {
        "id": "G_6NCWY6Mo8e",
        "outputId": "56110fea-b66b-431f-cd05-f005bf2e2df2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imbalanced-learn\n",
            "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.14.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
            "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m238.4/238.4 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: sklearn-compat, imbalanced-learn\n",
            "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Library\n",
        "import pandas as pd\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Bidirectional, BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "I6Pw9OLlHXG7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing dan Pembersihan"
      ],
      "metadata": {
        "id": "ENIdk__BiOJT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Membaca dan Menampilkan informasi dataset\n",
        "dataset = pd.read_csv(\"NDsO1LT_0lw_youtube_comments.csv\")\n",
        "print(\"Jumlah data:\", len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wtg0gpXcjEH8",
        "outputId": "cf1c62d5-9777-49a4-de84-3721ca5bda54"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jumlah data: 85028\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ADwNxWVjWUx",
        "outputId": "f0d3b850-6f1a-4b20-ac70-d445be2c4f4d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             username                                               text  \\\n",
            "0            @MrBeast  BEAST GAMES FINALE DROPS FEBRUARY 13TH! GO WAT...   \n",
            "1     @SultanRayyan-7                          Saya orang Indonesia ğŸ‡®ğŸ‡©ğŸ‡®ğŸ‡©   \n",
            "2  @MohamadrezaRezayy  Ù…Ø³ØªØ± Ø¨ÛŒØ³Øª Ø¹Ø²ÛŒØ² â¤ Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§ÛŒÙ† ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§...   \n",
            "3       @ILoveyou-954  à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦†à¦¸à¦¬à§‡à¦¨ à¦•à¦¬à§‡ à¦ªà¦¾à¦¬à¦¨à¦¾ à¦œà§‡à¦²à¦¾à¦° à¦•à¦¿à¦›à§ à¦¦à§‡à¦–à¦¤à§‡ à¦šà¦¾à¦‡ ...   \n",
            "4     @Moneymakerarab  This is for girls listen to Dalida - Helwa ya ...   \n",
            "\n",
            "   likes          published_at  \n",
            "0  54600  2025-02-08T16:59:31Z  \n",
            "1      0  2025-03-19T21:51:42Z  \n",
            "2      0  2025-03-19T21:43:03Z  \n",
            "3      0  2025-03-19T21:34:32Z  \n",
            "4      0  2025-03-19T21:20:21Z  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Informasi Dataset:\")\n",
        "print(dataset.info())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gD1WeBk3jj4g",
        "outputId": "389407e4-8355-4b8d-a53b-131fa52bd2a0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Informasi Dataset:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 85028 entries, 0 to 85027\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   username      84982 non-null  object\n",
            " 1   text          85028 non-null  object\n",
            " 2   likes         85028 non-null  int64 \n",
            " 3   published_at  85028 non-null  object\n",
            "dtypes: int64(1), object(3)\n",
            "memory usage: 2.6+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nStatistik Deskriptif:')\n",
        "print(dataset.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilYCEbrKjv5n",
        "outputId": "8d97b8e5-9c1a-44be-8beb-2c59360446e4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Statistik Deskriptif:\n",
            "               likes\n",
            "count   85028.000000\n",
            "mean        9.727654\n",
            "std       744.702565\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         1.000000\n",
            "max    167348.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek jumlah missing values di setiap kolom\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEaqaxe4j6Li",
        "outputId": "6ebdd490-d709-4aec-9c86-3ac776ac6f90"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "username        46\n",
            "text             0\n",
            "likes            0\n",
            "published_at     0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus nilai kosong\n",
        "dataset.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "ZTNC1ghpkkpa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Menghapus duplikasi\n",
        "dataset.drop_duplicates(subset=['text'], inplace=True)"
      ],
      "metadata": {
        "id": "b5HYvwY0kn00"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pembersihan teks\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'@\\w+', '', text)  # Menghapus username\n",
        "    text = re.sub(r'http\\S+', '', text)  # Menghapus URL\n",
        "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Menghapus karakter non-huruf\n",
        "    text = text.lower().strip()  # Mengubah ke huruf kecil dan menghapus spasi\n",
        "    return text\n",
        "\n",
        "dataset['cleaned_text'] = dataset['text'].apply(clean_text)\n",
        "print(\"Data setelah pembersihan:\")\n",
        "print(dataset.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mi_m65UkrwQ",
        "outputId": "fecad9af-15a1-4d68-a5cc-d0b57899c89f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data setelah pembersihan:\n",
            "             username                                               text  \\\n",
            "0            @MrBeast  BEAST GAMES FINALE DROPS FEBRUARY 13TH! GO WAT...   \n",
            "1     @SultanRayyan-7                          Saya orang Indonesia ğŸ‡®ğŸ‡©ğŸ‡®ğŸ‡©   \n",
            "2  @MohamadrezaRezayy  Ù…Ø³ØªØ± Ø¨ÛŒØ³Øª Ø¹Ø²ÛŒØ² â¤ Ù„Ø·ÙØ§ Ø¯ÙˆØ¨Ù„Ù‡ ÙØ§Ø±Ø³ÛŒ Ø§ÛŒÙ† ÙˆÛŒØ¯ÛŒÙˆ Ø±Ø§...   \n",
            "3       @ILoveyou-954  à¦¬à¦¾à¦‚à¦²à¦¾à¦¦à§‡à¦¶ à¦†à¦¸à¦¬à§‡à¦¨ à¦•à¦¬à§‡ à¦ªà¦¾à¦¬à¦¨à¦¾ à¦œà§‡à¦²à¦¾à¦° à¦•à¦¿à¦›à§ à¦¦à§‡à¦–à¦¤à§‡ à¦šà¦¾à¦‡ ...   \n",
            "4     @Moneymakerarab  This is for girls listen to Dalida - Helwa ya ...   \n",
            "\n",
            "   likes          published_at  \\\n",
            "0  54600  2025-02-08T16:59:31Z   \n",
            "1      0  2025-03-19T21:51:42Z   \n",
            "2      0  2025-03-19T21:43:03Z   \n",
            "3      0  2025-03-19T21:34:32Z   \n",
            "4      0  2025-03-19T21:20:21Z   \n",
            "\n",
            "                                        cleaned_text  \n",
            "0  beast games finale drops february th go watch ...  \n",
            "1                               saya orang indonesia  \n",
            "2                                                     \n",
            "3                                                     \n",
            "4  this is for girls listen to dalida  helwa ya b...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengecek kembali jumlah missing values di setiap kolom\n",
        "print(dataset.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocQOJSqMk5Nv",
        "outputId": "995a6118-809d-49d1-cca7-4fd0115da1ea"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "username        0\n",
            "text            0\n",
            "likes           0\n",
            "published_at    0\n",
            "cleaned_text    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Ekstraksi Fitur dan Pelabelan Data**"
      ],
      "metadata": {
        "id": "nBwOOPI5lASL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metode yang digunakan bebas sesuai dengan preferensi masing-masing peserta. Tahapan ini penting untuk mempersiapkan data sehingga dapat diolah lebih lanjut dalam proses pelatihan model."
      ],
      "metadata": {
        "id": "eJv-MtsUlHzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Melabeli komentar secara manual (positif, netral, negatif)\n",
        "def label_comment(text):\n",
        "    if any(word in text for word in [\"love\", \"great\", \"awesome\", \"good\", \"nice\", \"amazing\", \"fantastic\"]):\n",
        "        return \"positif\"\n",
        "    elif any(word in text for word in [\"hate\", \"bad\", \"awful\", \"worst\", \"terrible\", \"disgusting\", \"boring\"]):\n",
        "        return \"negatif\"\n",
        "    else:\n",
        "        return \"netral\"\n",
        "\n",
        "dataset['label'] = dataset['cleaned_text'].apply(label_comment)\n",
        "print(\"Distribusi label:\")\n",
        "print(dataset['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7Uvoet4lI9T",
        "outputId": "51fd54bc-4f3f-4802-af31-3e7f01193717"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi label:\n",
            "label\n",
            "netral     63770\n",
            "positif     4724\n",
            "negatif       69\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode label dengan LabelEncoder\n",
        "le = LabelEncoder()\n",
        "dataset['label'] = le.fit_transform(dataset['label'])"
      ],
      "metadata": {
        "id": "IqZS3KMplvFC"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Pembangunan Model Deep Learning**"
      ],
      "metadata": {
        "id": "97mjHYpAmZKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pilihan algoritma pelatihan ini haruslah sesuai dengan tujuan analisis sentimen yang ingin dicapai."
      ],
      "metadata": {
        "id": "lXa9RkwLmd-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Skema 3: GRU dengan Word2Vec (70/30)"
      ],
      "metadata": {
        "id": "Vp1D360LqVIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Algoritma:** GRU (Gated Recurrent Unit)\n",
        "\n",
        "**Ekstraksi Fitur:** Word2Vec\n",
        "\n",
        "**Alasan Pemilihan:**\n",
        "\n",
        "- GRU lebih sederhana dan lebih ringan dibandingkan dengan LSTM, sehingga lebih cepat dalam proses pelatihan tanpa kehilangan banyak informasi temporal. Cocok untuk dataset teks dengan jumlah data besar.\n",
        "- Word2Vec digunakan untuk menghasilkan vektor kata yang memiliki makna semantik sehingga model dapat memahami konteks dari kata-kata dalam kalimat."
      ],
      "metadata": {
        "id": "0_wEtFsD-ODX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nSkema 3: GRU dengan Word2Vec (70/30)\")\n",
        "# Split data 70/30\n",
        "X = dataset['cleaned_text']\n",
        "y = dataset['label']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "2FyX2y7Fqd1i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e69a0d1-f2f5-488f-8118-2e3a3aadfb37"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Skema 3: GRU dengan Word2Vec (70/30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizer dan Padding\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(X_train)  # Hanya fit pada data latih untuk mencegah data bocor\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "# Padding\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=209, padding='post')\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=209, padding='post')"
      ],
      "metadata": {
        "id": "_j0tircbPPiN"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mengatasi Ketidakseimbangan Data dengan SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_padded, y_train)\n",
        "\n",
        "print(\"Distribusi label setelah SMOTE:\")\n",
        "print(pd.Series(y_resampled).value_counts())"
      ],
      "metadata": {
        "id": "sIaTRLE6htne",
        "outputId": "19f171ae-a97e-40e7-9164-1e029796ef62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Distribusi label setelah SMOTE:\n",
            "label\n",
            "1    44619\n",
            "2    44619\n",
            "0    44619\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding pada label\n",
        "encoder = LabelEncoder()\n",
        "y_resampled_encoded = encoder.fit_transform(y_resampled)  # Transform label SMOTE\n",
        "y_test_encoded = encoder.transform(y_test)  # Transform label uji\n"
      ],
      "metadata": {
        "id": "brElYiJWPXb6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding\n",
        "y_resampled_encoded = tf.keras.utils.to_categorical(y_resampled_encoded, num_classes=3)\n",
        "y_test_encoded = tf.keras.utils.to_categorical(y_test_encoded, num_classes=3)\n",
        "\n",
        "print(\"Label Setelah One-Hot Encoding:\")\n",
        "print(y_resampled_encoded[:5])\n",
        "print(\"Classes:\", encoder.classes_)"
      ],
      "metadata": {
        "id": "sKFQ6SspUwCp",
        "outputId": "5bfe1e66-3c29-45ec-8618-d53c0471e520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label Setelah One-Hot Encoding:\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n",
            "Classes: [0 1 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model GRU\n",
        "model3 = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=209),  # Embedding layer lebih kecil\n",
        "    Bidirectional(GRU(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)),  # GRU lebih kecil\n",
        "    GlobalMaxPooling1D(),  # Pooling\n",
        "    BatchNormalization(),  # Normalisasi batch\n",
        "    Dropout(0.3),  # Dropout lebih rendah\n",
        "    Dense(64, activation='relu'),  # Layer Dense lebih kecil\n",
        "    Dropout(0.3),  # Dropout lebih rendah\n",
        "    Dense(32, activation='relu'),  # Layer Dense lebih kecil\n",
        "    Dropout(0.3),  # Dropout lagi\n",
        "    Dense(3, activation='softmax')  # Output layer dengan 3 kelas\n",
        "])"
      ],
      "metadata": {
        "id": "YZFjHTkWqeuk"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model3.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "oglGMxxdBOoD"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Melatih model dengan data seimbang\n",
        "history = model3.fit(\n",
        "    X_resampled, y_resampled_encoded,  # Data dan label sudah One-Hot\n",
        "    epochs=10,\n",
        "    validation_data=(X_test_padded, y_test_encoded),  # Data validasi juga One-Hot\n",
        "    batch_size=64\n",
        ")"
      ],
      "metadata": {
        "id": "wurPNkJ4XHHs",
        "outputId": "8d75575d-e39d-4ad0-c080-2aceb4412673",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1219s\u001b[0m 583ms/step - accuracy: 0.9227 - loss: 0.2071 - val_accuracy: 0.9725 - val_loss: 0.1052\n",
            "Epoch 2/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1227s\u001b[0m 585ms/step - accuracy: 0.9344 - loss: 0.1814 - val_accuracy: 0.9698 - val_loss: 0.1081\n",
            "Epoch 3/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1205s\u001b[0m 576ms/step - accuracy: 0.9424 - loss: 0.1597 - val_accuracy: 0.9700 - val_loss: 0.1204\n",
            "Epoch 4/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1202s\u001b[0m 575ms/step - accuracy: 0.9476 - loss: 0.1458 - val_accuracy: 0.9578 - val_loss: 0.1437\n",
            "Epoch 5/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1219s\u001b[0m 573ms/step - accuracy: 0.9534 - loss: 0.1327 - val_accuracy: 0.9736 - val_loss: 0.1010\n",
            "Epoch 6/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1254s\u001b[0m 589ms/step - accuracy: 0.9583 - loss: 0.1194 - val_accuracy: 0.9632 - val_loss: 0.1492\n",
            "Epoch 7/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1259s\u001b[0m 578ms/step - accuracy: 0.9602 - loss: 0.1150 - val_accuracy: 0.9736 - val_loss: 0.1047\n",
            "Epoch 8/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1197s\u001b[0m 566ms/step - accuracy: 0.9639 - loss: 0.1052 - val_accuracy: 0.9632 - val_loss: 0.1349\n",
            "Epoch 9/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1170s\u001b[0m 559ms/step - accuracy: 0.9658 - loss: 0.0983 - val_accuracy: 0.9675 - val_loss: 0.1386\n",
            "Epoch 10/10\n",
            "\u001b[1m2092/2092\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1214s\u001b[0m 556ms/step - accuracy: 0.9685 - loss: 0.0911 - val_accuracy: 0.9705 - val_loss: 0.1223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data testing\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=209, padding='post')\n",
        "\n",
        "# Evaluasi model pada data testing\n",
        "test_loss, test_accuracy = model3.evaluate(X_test_padded, y_test_encoded)\n",
        "print(f\"Akurasi Testing (dari model.evaluate): {test_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "8t5QY8K06muf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a2013c0-bb95-4ed9-a2c6-8f3b123cb555"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m643/643\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 80ms/step - accuracy: 0.9723 - loss: 0.1182\n",
            "Akurasi Testing (dari model.evaluate): 97.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = [str(cls) for cls in le.classes_]\n",
        "print(target_names)  # Pastikan hasilnya dalam bentuk list of strings"
      ],
      "metadata": {
        "id": "H3Bgox8yfpT8",
        "outputId": "49b08d5a-8688-4f5f-bd38-964f4a44fdce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0', '1', '2']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(le.classes_))\n",
        "print(le.classes_)\n",
        "print(list(le.classes_))"
      ],
      "metadata": {
        "id": "LSJQ4dOTfe5O",
        "outputId": "7174eca4-68df-48e8-ffa5-aec36fce8678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "[0 1 2]\n",
            "[np.int64(0), np.int64(1), np.int64(2)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediksi pada data testing\n",
        "y_pred2 = np.argmax(model3.predict(X_test_padded), axis=1)\n",
        "y_true = np.argmax(y_test_encoded, axis=1)\n",
        "\n",
        "print(classification_report(y_true, y_pred2, target_names=target_names))"
      ],
      "metadata": {
        "id": "JglADpySeOM2",
        "outputId": "507b09f7-cdde-43ba-b635-27eab956c4e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m643/643\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 81ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.02      0.42      0.04        12\n",
            "           1       1.00      0.97      0.98     19151\n",
            "           2       0.80      0.97      0.87      1406\n",
            "\n",
            "    accuracy                           0.97     20569\n",
            "   macro avg       0.61      0.78      0.63     20569\n",
            "weighted avg       0.98      0.97      0.98     20569\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inference  Skema 3\n",
        "print(\"\\nInference Skema 3: GRU dengan Word2Vec (70/30)\")\n",
        "def predict_text(text, tokenizer, model3, le, max_length=100):\n",
        "    # Preprocessing teks\n",
        "    text = text.lower()\n",
        "    text_seq = tokenizer.texts_to_sequences([text])\n",
        "    text_pad = pad_sequences(text_seq, maxlen=max_length, padding='post')\n",
        "\n",
        "    # Prediksi\n",
        "    prediction = model3.predict(text_pad)\n",
        "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
        "    predicted_class = le.inverse_transform([predicted_label])[0]\n",
        "\n",
        "    print(f\"Teks: {text}\")\n",
        "    print(f\"Prediksi Label: {predicted_class}\")\n",
        "    print(f\"Confidence: {np.max(prediction) * 100:.2f}%\")\n",
        "    return predicted_class"
      ],
      "metadata": {
        "id": "9xQu21Vcqhwt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "164ca365-ebf1-452d-a01a-1fcdcfd6fc2d"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Inference Skema 3: GRU dengan Word2Vec (70/30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Melakukan Prediksi pada Teks Baru (0: Negatif, 1: Netral, 2: Positif)\n",
        "predict_text(\"I love how you guys taking the video, that was amazing!\", tokenizer, model3, le)"
      ],
      "metadata": {
        "id": "8o4sUiuhubdX",
        "outputId": "81f33197-c701-423a-8c40-ac5d1f3e5ae0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "Teks: i love how you guys taking the video, that was amazing!\n",
            "Prediksi Label: 2\n",
            "Confidence: 100.00%\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insight Skema 3:**\n",
        "\n",
        "1. Model memiliki akurasi tinggi sebesar 97.05%, yang menunjukkan bahwa model secara keseluruhan cukup andal dalam mengklasifikasikan data.\n",
        "2. **Kelas 0 (Negatif):**\n",
        "\n",
        "- Precision: 0.02 (Sangat rendah)\n",
        "\n",
        "- Recall: 0.42 (Cukup tinggi untuk jumlah data yang sedikit)\n",
        "\n",
        "- F1-Score: 0.04 (Buruk)\n",
        "\n",
        "- Jumlah data: 12 (Sangat sedikit)\n",
        "\n",
        "- **Catatan:** Model mengalami kesulitan dalam mengklasifikasikan kelas ini karena jumlah sampel yang sangat sedikit sehingga model cenderung mengabaikannya.\n",
        "\n",
        "3. **Kelas 1 (Netral):**\n",
        "\n",
        "- Precision: 1.00 (Sangat baik)\n",
        "\n",
        "- Recall: 0.97 (Sangat baik)\n",
        "\n",
        "- F1-Score: 0.98 (Sangat tinggi)\n",
        "\n",
        "- Jumlah data: 19,151 (Mayoritas data)\n",
        "\n",
        "- **Catatan:** Model sangat akurat dalam mendeteksi kelas ini karena data yang melimpah\n",
        "\n",
        "4. **Kelas 2 (Positif):**\n",
        "\n",
        "- Precision: 0.80 (Baik)\n",
        "\n",
        "- Recall: 0.97 (Sangat baik)\n",
        "\n",
        "- F1-Score: 0.87 (Tinggi)\n",
        "\n",
        "- Jumlah data: 1,406 (Cukup banyak)\n",
        "\n",
        "- **Catatan**: Model juga mampu menangani kelas positif dengan cukup baik.\n",
        "\n",
        "5. Model memprediksi teks \"i love how you guys taking the video, that was amazing!\" sebagai kelas 2 (Positif) dengan 100% confidence."
      ],
      "metadata": {
        "id": "YyASQxddzH38"
      }
    }
  ]
}